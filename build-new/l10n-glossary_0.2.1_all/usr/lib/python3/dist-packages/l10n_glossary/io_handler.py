#!/usr/bin/env python3
# SPDX-License-Identifier: GPL-3.0-or-later
# Copyright 2026 Daniel Nylander <daniel@danielnylander.se>

"""I/O handlers for glossary files (TBX, CSV, TSV) and import from PO/TS."""

import csv
import os
import re
import gettext
from collections import Counter
from lxml import etree

from l10n_glossary.glossary import Glossary, Term

_ = gettext.gettext


def load_glossary(path):
    """Load a glossary from file. Detects format by extension."""
    ext = os.path.splitext(path)[1].lower()
    if ext == ".tbx":
        return _load_tbx(path)
    elif ext == ".csv":
        return _load_delimited(path, ",")
    elif ext == ".tsv":
        return _load_delimited(path, "\t")
    else:
        raise ValueError(_("Unsupported file format: {}").format(ext))


def save_glossary(glossary, path, fmt=None):
    """Save glossary to file. Format detected from extension or fmt param."""
    if fmt is None:
        fmt = os.path.splitext(path)[1].lstrip(".").lower()
    if fmt == "tbx":
        _save_tbx(glossary, path)
    elif fmt == "csv":
        _save_delimited(glossary, path, ",")
    elif fmt == "tsv":
        _save_delimited(glossary, path, "\t")
    else:
        raise ValueError(_("Unsupported format: {}").format(fmt))


def _load_tbx(path):
    """Load TBX glossary."""
    tree = etree.parse(path)
    root = tree.getroot()
    ns = {"tbx": "urn:iso:std:iso:30042:ed-2"}
    glossary = Glossary()

    # Try both namespaced and non-namespaced
    entries = root.findall(".//tbx:termEntry", ns)
    if not entries:
        entries = root.findall(".//termEntry")

    for entry in entries:
        source = ""
        targets = []

        lang_sets = entry.findall("tbx:langSet", ns)
        if not lang_sets:
            lang_sets = entry.findall("langSet")

        for lang_set in lang_sets:
            lang = lang_set.get("{http://www.w3.org/XML/1998/namespace}lang", "")

            tigs = lang_set.findall("tbx:tig", ns)
            if not tigs:
                tigs = lang_set.findall("tig")
            if not tigs:
                tigs = lang_set.findall("tbx:ntig", ns)
            if not tigs:
                tigs = lang_set.findall("ntig")

            for tig in tigs:
                term_el = tig.find("tbx:term", ns)
                if term_el is None:
                    term_el = tig.find("term")
                    if term_el is None:
                        # Try termGrp/term
                        tg = tig.find("tbx:termGrp", ns)
                        if tg is None:
                            tg = tig.find("termGrp")
                        if tg is not None:
                            term_el = tg.find("tbx:term", ns)
                            if term_el is None:
                                term_el = tg.find("term")

                if term_el is not None:
                    text = term_el.text or ""
                    if not source:
                        source = text
                    else:
                        targets.append((text, lang))

        for target, lang in targets:
            glossary.terms.append(Term(source=source, target=target, language=lang))

    return glossary


def _save_tbx(glossary, path):
    """Save as TBX."""
    root = etree.Element("tbx", type="TBX-Basic")
    root.set("xmlns", "urn:iso:std:iso:30042:ed-2")
    header = etree.SubElement(root, "tbxHeader")
    file_desc = etree.SubElement(header, "fileDesc")
    source_desc = etree.SubElement(file_desc, "sourceDesc")
    p = etree.SubElement(source_desc, "p")
    p.text = "Generated by Glossary Editor"

    body = etree.SubElement(root, "text")
    body_el = etree.SubElement(body, "body")

    # Group by source term
    by_source = {}
    for term in glossary.terms:
        by_source.setdefault(term.source, []).append(term)

    for source, terms in by_source.items():
        entry = etree.SubElement(body_el, "termEntry")

        # Source langSet (English assumed)
        src_ls = etree.SubElement(entry, "langSet")
        src_ls.set("{http://www.w3.org/XML/1998/namespace}lang", "en")
        src_tig = etree.SubElement(src_ls, "tig")
        src_term = etree.SubElement(src_tig, "term")
        src_term.text = source

        for t in terms:
            tgt_ls = etree.SubElement(entry, "langSet")
            tgt_ls.set("{http://www.w3.org/XML/1998/namespace}lang",
                       t.language or "und")
            tgt_tig = etree.SubElement(tgt_ls, "tig")
            tgt_term = etree.SubElement(tgt_tig, "term")
            tgt_term.text = t.target
            if t.comment:
                note = etree.SubElement(tgt_tig, "note")
                note.text = t.comment

    tree = etree.ElementTree(root)
    tree.write(path, xml_declaration=True, encoding="UTF-8", pretty_print=True)


def _load_delimited(path, delimiter):
    """Load CSV/TSV glossary."""
    glossary = Glossary()
    with open(path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f, delimiter=delimiter)
        for row in reader:
            glossary.terms.append(Term(
                source=row.get("source", row.get("Source", "")),
                target=row.get("target", row.get("Target", "")),
                language=row.get("language", row.get("Language", "")),
                context=row.get("context", row.get("Context", "")),
                comment=row.get("comment", row.get("Comment", "")),
            ))
    return glossary


def _save_delimited(glossary, path, delimiter):
    """Save as CSV/TSV."""
    with open(path, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(
            f, fieldnames=["source", "target", "language", "context", "comment"],
            delimiter=delimiter)
        writer.writeheader()
        for term in glossary.terms:
            writer.writerow({
                "source": term.source,
                "target": term.target,
                "language": term.language,
                "context": term.context,
                "comment": term.comment,
            })


def import_po_terms(path, min_frequency=1):
    """Import terms from a .po file. Extracts msgid/msgstr pairs."""
    terms = []
    with open(path, "r", encoding="utf-8") as f:
        content = f.read()

    # Parse PO entries
    entries = re.findall(
        r'msgid\s+"((?:[^"\\]|\\.)*)"\s*\n(?:.*?\n)*?msgstr\s+"((?:[^"\\]|\\.)*)"',
        content)

    # Try to detect language from header
    lang_match = re.search(r'Language:\s*(\S+)', content)
    lang = lang_match.group(1) if lang_match else ""

    for msgid, msgstr in entries:
        if msgid and msgstr:  # Skip empty entries
            # Unescape
            msgid = msgid.replace('\\n', '\n').replace('\\"', '"')
            msgstr = msgstr.replace('\\n', '\n').replace('\\"', '"')
            terms.append(Term(source=msgid, target=msgstr, language=lang))

    return terms


def import_ts_terms(path):
    """Import terms from a Qt .ts file."""
    terms = []
    tree = etree.parse(path)
    root = tree.getroot()

    lang = root.get("language", "")

    for message in root.findall(".//message"):
        source_el = message.find("source")
        trans_el = message.find("translation")
        if source_el is not None and trans_el is not None:
            source = source_el.text or ""
            target = trans_el.text or ""
            if source and target:
                ctx_el = message.getparent()
                context = ""
                if ctx_el is not None:
                    name_el = ctx_el.find("name")
                    if name_el is not None:
                        context = name_el.text or ""
                terms.append(Term(
                    source=source, target=target,
                    language=lang, context=context))

    return terms
